# -*- coding: utf-8 -*-
"""Yenigithubdataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tGPjfXSfc9zrxUaLesU5j6131WHHKkHV
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

from IPython.display import FileLink
from kaggle.api.kaggle_api_extended import KaggleApi
import os
from google.colab import drive
import shutil # Import shutil for moving files

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Authenticate with Kaggle
# Ensure your kaggle.json is set up correctly as described in the previous step
api = KaggleApi()
api.authenticate()

# Define the dataset identifier on Kaggle
kaggle_dataset_id = 'muneebullah123/embedded-cc-raw-code'

# Define a temporary local path for the download in the Colab environment
temp_local_path = '/content/temp_kaggle_download/'

# Define the final destination path on Google Drive
drive_destination_path = '/content/drive/MyDrive/embedded_cc_raw_code/' # Changed to a new folder name

# Create the temporary local directory if it doesn't exist
os.makedirs(temp_local_path, exist_ok=True)

# Create the final destination directory on Google Drive if it doesn't exist
os.makedirs(drive_destination_path, exist_ok=True)


# Download the dataset archive to the temporary local path
print(f"Downloading dataset archive to temporary path: {temp_local_path}")
try:
    api.dataset_download_files(
        dataset=kaggle_dataset_id,
        path=temp_local_path,
        force=True,
        unzip=True # Unzip directly into the temporary folder
    )
    print(f"Dataset archive downloaded and unzipped to: {temp_local_path}")

    # Move the extracted contents from the temporary path to Google Drive
    print(f"Moving extracted contents to Google Drive: {drive_destination_path}")
    for item in os.listdir(temp_local_path):
        s = os.path.join(temp_local_path, item)
        d = os.path.join(drive_destination_path, item)
        if os.path.isdir(s):
            shutil.move(s, d) # Move directories
        else:
            shutil.move(s, d) # Move files

    print("Successfully moved files to Google Drive.")

    # Clean up the temporary directory
    shutil.rmtree(temp_local_path)
    print(f"Cleaned up temporary directory: {temp_local_path}")

    # You can optionally list the contents of the destination folder on Drive
    print(f"Contents of {drive_destination_path}:")
    !ls {drive_destination_path}


except Exception as e:
    print(f"Error during download or move: {e}")

# Note: FileLink is less useful for directories, but you can create links for individual files if needed.
# For example, to link to a specific file after moving:
# display(FileLink(os.path.join(drive_destination_path, 'your_file_name.ext')))

import os

def split_file_by_lines(input_path, output_dir, lines_per_file=1_000_000, prefix='filtered_embedded_code.txt'):

    os.makedirs(output_dir, exist_ok=True)
    file_idx = 1
    line_count = 0
    out_handle = None

    with open(input_path, 'r', encoding='utf-8', errors='ignore') as inp:
        for line in inp:
            # Yeni par√ßa a√ßmak gerekirse
            if line_count % lines_per_file == 0:
                if out_handle:
                    out_handle.close()
                part_name = f"{prefix}_{file_idx:03d}.txt"
                part_path = os.path.join(output_dir, part_name)
                print(f"‚Üí A√ßƒ±lƒ±yor: {part_path}")
                out_handle = open(part_path, 'w', encoding='utf-8')
                file_idx += 1
            out_handle.write(line)
            line_count += 1

        # Son par√ßayƒ± kapat
        if out_handle:
            out_handle.close()

    print(f"\nüü¢ B√∂lme tamamlandƒ±! Toplam satƒ±r: {line_count}, par√ßalar: {file_idx-1}")

# ---------- Kullanƒ±m ----------

combined_path = '/content/drive/MyDrive/filtered_embedded_code_combined.txt'
split_dir     = '/content/drive/MyDrive/filtered_parts'

split_file_by_lines(
    input_path     = combined_path,
    output_dir     = split_dir,
    lines_per_file = 1_000_000,
    prefix         = 'filtered_embedded_code.txt'
)

import os

def format_size(num_bytes):
    """
    Bayt cinsinden gelen sayƒ±yƒ± KB, MB, GB olarak bi√ßimlendirir.
    """
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if num_bytes < 1024.0:
            return f"{num_bytes:.1f}{unit}"
        num_bytes /= 1024.0
    return f"{num_bytes:.1f}PB"

def print_folder_structure(root_dir, prefix=''):
    """
    root_dir i√ßindeki klas√∂r ve dosyalarƒ± aƒüa√ß formatƒ±nda ve dosya boyutlarƒ±yla yazdƒ±rƒ±r.
    prefix: her seviyede girinti i√ßin kullanƒ±lƒ±r.
    """
    entries = sorted(os.listdir(root_dir))
    for idx, entry in enumerate(entries):
        path = os.path.join(root_dir, entry)
        is_last = (idx == len(entries) - 1)
        connector = '‚îî‚îÄ‚îÄ ' if is_last else '‚îú‚îÄ‚îÄ '

        if os.path.isfile(path):
            size = os.path.getsize(path)
            size_str = format_size(size)
            print(prefix + connector + f"{entry} ({size_str})")
        else:
            # Klas√∂rse, sadece adƒ± yaz, boyut yerine "<DIR>" ekleyebilirsin
            print(prefix + connector + f"{entry}/")
            # Alt klas√∂r i√ßin girintiyi hazƒ±rla ve recursive √ßaƒüƒ±r
            extension = '    ' if is_last else '‚îÇ   '
            print_folder_structure(path, prefix + extension)

if __name__ == '__main__':
    root_folder = drive_destination_path
    print(root_folder)
    print_folder_structure(root_folder)

rm *.txt

import re
import gc
import os
import time

# ==============================================================================
# == OUTPUT MANAGER CLASS ==
# ==============================================================================
class OutputManager:
    """Manages writing output to multiple smaller files."""

    def __init__(self, base_filename, max_lines_per_file=1_000_000):
        self.base_filename, self.ext = os.path.splitext(base_filename)
        self.max_lines_per_file = max_lines_per_file
        self.current_file_index = 0
        self.current_line_count = 0
        self.current_handle = None
        self._open_new_file() # Open the first file

    def _open_new_file(self):
        """Closes the current file (if any) and opens a new one."""
        if self.current_handle:
            try:
                self.current_handle.close()
            except Exception as e:
                print(f"\n[Warning] Could not close previous file: {e}", flush=True)

        self.current_file_index += 1
        self.current_line_count = 0
        new_filename = f"{self.base_filename}_{self.current_file_index:03d}{self.ext}"
        print(f"\n[Output] Opening new output file: {new_filename}", flush=True)
        try:
            self.current_handle = open(new_filename, 'w', encoding='utf-8')
        except Exception as e:
            print(f"\n[FATAL ERROR] Could not open output file {new_filename}: {e}", flush=True)
            raise # Re-raise the exception to stop the script

    def write(self, block_text, score, source_file):
        """Writes a block of text, switching files if necessary."""
        if not self.current_handle:
            print("\n[ERROR] Output file handle is not open!", flush=True)
            return 0 # Return 0 lines written

        lines_in_block = block_text.count('\n') + 1

        if (self.current_line_count + lines_in_block) > self.max_lines_per_file and self.current_line_count > 0:
            self._open_new_file()

        try:
            self.current_handle.write(f"--- START CCB (Score: {score}, File: {source_file}) ---\n")
            self.current_handle.write(block_text)
            self.current_handle.write("--- END CCB ---\n\n")
            self.current_line_count += (lines_in_block + 3) # Add lines for text + delimiters
            return lines_in_block # Return lines *of code* written
        except Exception as e:
            print(f"\n[ERROR] Could not write to output file: {e}", flush=True)
            return 0

    def close(self):
        """Closes the final output file."""
        if self.current_handle:
            print(f"\n[Output] Closing final output file.", flush=True)
            try:
                self.current_handle.close()
            except Exception as e:
                print(f"\n[Warning] Could not close final file: {e}", flush=True)
        self.current_handle = None

# ==============================================================================
# == INDICATORS (Unchanged - keep as before) ==
# ==============================================================================
# ==============================================================================
# == MORE STRINGENT INDICATOR PATTERNS AND WEIGHTS ==
# ==============================================================================
INDICATORS = {
    # --- Strong Positive Indicators (Higher Weights) ---
    r'#include\s*<\s*avr/.*>': 100,           # Atmel AVR MCUs
    r'#include\s*<\s*stm32.*>': 100,          # STMicroelectronics ARM Cortex-M MCUs
    r'#include\s*<\s*msp430.*>': 100,         # Texas Instruments MCUs
    r'#include\s*<\s*(pic|htc|xc)\.h\s*>': 100, # Microchip PIC MCUs
    r'#include\s*<\s*(NXP|LPC).*?>': 100,      # NXP MCUs
    r'#include\s*<\s*arm_math\.h\s*>': 70,     # ARM CMSIS Math Library
    r'#include\s*<\s*FreeRTOS\.h\s*>': 80,       # FreeRTOS Core
    r'#include\s*<\s*task\.h\s*>': 80,           # FreeRTOS Tasks
    r'#include\s*<\s*ucos_ii\.h\s*>': 80,      # Micrium ¬µC/OS-II
    r'#include\s*<\s*cmsis_os\.h\s*>': 80,       # ARM CMSIS-OS
    r'\bISR\s*\(': 70,                      # Interrupt Service Routine (common macro)
    r'\b__interrupt\b': 70,                  # Interrupt keyword (compiler specific)
    r'#\s*pragma\s+vector': 60,          # IAR-style interrupt vector
    r'#\s*pragma\s+interrupt': 60,       # Compiler-specific interrupt pragma
    r'\b(sfr|sbit)\b': 60,                # 8051/PIC style register keywords
    r'\*\s*\(\s*volatile.*\)\s*0x[4-9A-Fa-f][0-9A-Fa-f]{3,7}': 75, # MMI/O Access (Higher Weight)
    r'\b__attribute__\s*\(\s*\(.*(section|interrupt).*\)\s*\)': 40, # GCC specific (sections, interrupts)
    r'\b__weak\b': 25,                       # Weak linkage (common in HALs/RTOS)

    # --- Medium Positive Indicators (Lower/Adjusted Weights) ---
    r'\bvolatile\b': 5,                  # Volatile keyword (Lower Weight)
    r'while\s*\(\s*1\s*\)': 10,          # Common endless loop (Lower Weight)
    r'for\s*\(\s*;\s*;\s*\)': 10,          # Alternative endless loop (Lower Weight)
    r'\b(PORT|GPIO|DDR|PIN)[A-Z_0-9]+\b': 20,  # Common GPIO (More specific, Higher)
    r'\b(ADC|SPI|I2C|UART|CAN|TIM|WDT|IWDG)[0-9_]*': 15, # Peripherals (More specific, Higher)
    r'#include\s*<stdint\.h>': 1,        # Standard int types (Much Lower Weight)
    r'#include\s*<stdbool\.h>': 1,       # Standard bool types (Much Lower Weight)
    r'[|&^~]': 1,                        # Bitwise operations (Lower Weight)
    r'(<<|>>)': 1,                       # Bit shift operations (Lower Weight)

    # --- Negative Indicators (Much Higher Weights) ---
    r'#include\s*<windows\.h>': -500,      # Windows API
    r'#include\s*<X11/Xlib\.h>': -500,    # X11 GUI API
    r'#include\s*<gtk/gtk\.h>': -500,      # GTK GUI Toolkit
    r'#include\s*<Qt.*>': -500,            # Qt GUI Toolkit
    r'#include\s*<(winsock2|sys/socket)\.h>': -300, # Desktop/Server Networking
    r'#include\s*<(GL/gl|OpenGL/.*)\.h>': -600, # Graphics Libraries
    r'#include\s*<pthread\.h>': -150,      # POSIX Threads
    r'#include\s*<boost/.*>': -100,       # Boost C++ Libraries
    r'\bfopen\b': -60,                   # File I/O
    r'\bfwrite\b': -60,                  # File I/O
    r'\bfread\b': -60,                   # File I/O
    r'#include\s*<iostream>': -40,       # C++ I/O streams
    r'#include\s*<vector>': -40,          # C++ Vector
    r'#include\s*<string>': -40,          # C++ String
    r'#include\s*<stdio\.h>': -10,        # Standard IO (small penalty, often used for debug UART)
    r'\bprintf\b': -15,                  # Console I/O
    r'\bscanf\b': -30,                   # Console Input
    r'System\.Windows\.Forms': -1000,    # .NET GUI
    r'javax\.swing': -1000,              # Java GUI
    r'#include\s*<unistd\.h>': -100,       # POSIX Standard
}

# --- Compile Regexes for Performance ---
COMPILED_INDICATORS = {re.compile(k, re.IGNORECASE): v for k, v in INDICATORS.items()}

# ==============================================================================
# == FILTERING CODE (Modified) ==
# ==============================================================================
def calculate_score(ccb_text):
    score = 0
    for pattern, weight in COMPILED_INDICATORS.items():
        occurrences = pattern.findall(ccb_text)
        score += len(occurrences) * weight
    return score

def process_file(filepath, threshold, output_manager):
    lines_written_this_file = 0
    buffer = []
    BLOCK_SIZE = 500
    OVERLAP = 100
    PROGRESS_UPDATE_INTERVAL = 200
    block_count = 0
    source_filename = os.path.basename(filepath)

    try:
        file_size = os.path.getsize(filepath)
        start_time = time.time()

        with open(filepath, 'r', encoding='utf-8', errors='ignore') as infile:
            while True:
                new_lines = []
                for _ in range(BLOCK_SIZE if not buffer else BLOCK_SIZE - OVERLAP):
                    line = infile.readline()
                    if not line: break
                    new_lines.append(line)

                if not new_lines and not buffer: break
                buffer = buffer[len(buffer) - OVERLAP:] + new_lines if buffer and OVERLAP < len(buffer) else new_lines
                if not buffer: break

                block_count += 1
                ccb_text = "".join(buffer)
                score = calculate_score(ccb_text)

                if score >= threshold:
                    # Use the OutputManager to write
                    lines_written = output_manager.write(ccb_text, score, source_filename)
                    lines_written_this_file += lines_written

                # Progress Update
                if block_count % PROGRESS_UPDATE_INTERVAL == 0:
                    current_pos = infile.tell()
                    elapsed_time = time.time() - start_time
                    percent_done = (current_pos / file_size) * 100 if file_size > 0 else 100
                    speed_mb_s = (current_pos / (1024 * 1024)) / elapsed_time if elapsed_time > 0 else 0
                    print(f"  -> {source_filename}: {percent_done:.1f}% done ({speed_mb_s:.2f} MB/s)", end='\r', flush=True)

                if not new_lines: break

            print(f"  -> {source_filename}: 100.0% done.                          ", flush=True)

    except FileNotFoundError:
        print(f"  -> ERROR: File not found {filepath}", flush=True)
        return 0
    except Exception as e:
        print(f"  -> ERROR processing {filepath}: {e}", flush=True)
        return 0

    return lines_written_this_file # Return lines *of code* extracted

def run_filter(input_dir, base_output_name, threshold=50, max_lines_per_file=1_000_000):
    input_files = []
    extensions = ('.c', '.cpp', '.cxx', '.h', '.hpp', '.hxx')
    print(f"Searching for source files in: {input_dir}")
    for root, _, files in os.walk(input_dir):
        for file in files:
            if file.lower().endswith(extensions):
                input_files.append(os.path.join(root, file))

    print(f"Found {len(input_files)} source files.")
    if not input_files: return

    print(f"\nStarting filter with Threshold = {threshold}")
    print(f"Output base name: {base_output_name}")
    print(f"Max lines per file: {max_lines_per_file}")
    print("-" * 50)

    total_extracted_lines = 0
    start_time_total = time.time()

    # Create the Output Manager
    output_manager = OutputManager(base_output_name, max_lines_per_file)

    try:
        for i, f_path in enumerate(input_files):
            print(f"Processing file {i + 1} of {len(input_files)}: {os.path.basename(f_path)}...")
            lines_from_file = process_file(f_path, threshold, output_manager)
            total_extracted_lines += lines_from_file
            print(f"  -> Extracted: {lines_from_file} | Total Extracted: {total_extracted_lines}", flush=True)
    finally:
        # Ensure the last file is closed, even if an error occurs
        output_manager.close()

    end_time_total = time.time()
    total_duration_minutes = (end_time_total - start_time_total) / 60

    print(f"\n========================================================")
    print(f"Filtering Complete!")
    print(f"Total time taken: {total_duration_minutes:.2f} minutes.")
    print(f"Total lines extracted with Threshold {threshold}: {total_extracted_lines}")
    print(f"Output files are named '{base_output_name}_NNN.txt'.")
    print(f"========================================================")

# ==============================================================================
# == HOW TO RUN ==
# ==============================================================================
YOUR_INPUT_DIRECTORY = '/content/drive/MyDrive/embedded_cc_raw_code/embedded-c-p1/embedded-c-p1/'
YOUR_OUTPUT_BASENAME = 'filtered_embedded_code.txt' # Will become _001.txt, _002.txt ...
INITIAL_THRESHOLD = 90
LINES_PER_OUTPUT_FILE = 1_000_000 # Set desired lines per file

# --- Uncomment the line below to run ---
run_filter(YOUR_INPUT_DIRECTORY, YOUR_OUTPUT_BASENAME, INITIAL_THRESHOLD, LINES_PER_OUTPUT_FILE)

print("Script loaded. Adjust parameters and uncomment 'run_filter' to start.")

import json
import os

# 1) Buraya sadece istediƒüin par√ßalarƒ±n tam yollarƒ±nƒ± ekle
files_to_process = [
    '/content/drive/MyDrive/filtered_parts/filtered_embedded_code.txt_001.txt',
'/content/drive/MyDrive/filtered_parts/filtered_embedded_code.txt_020.txt',
'/content/drive/MyDrive/filtered_parts/filtered_embedded_code.txt_040.txt',
'/content/drive/MyDrive/filtered_parts/filtered_embedded_code.txt_060.txt',
'/content/drive/MyDrive/filtered_parts/filtered_embedded_code.txt_064.txt',
'/content/drive/MyDrive/filtered_parts/filtered_embedded_code.txt_083.txt',
'/content/drive/MyDrive/filtered_parts/filtered_embedded_code.txt_098.txt',
'/content/drive/MyDrive/filtered_parts/filtered_embedded_code.txt_100.txt',

]

# 2) √áƒ±ktƒ± JSONL dosyanƒ±n yolu
output_path = '/content/drive/MyDrive/filtered_selected_embedded_code.jsonl'

# 3) Var olanƒ± sil (opsiyonel)
if os.path.exists(output_path):
    os.remove(output_path)

# 4) Her satƒ±rƒ± {"code": "..."} formatƒ±nda JSONL‚Äôe yaz
with open(output_path, 'w', encoding='utf-8') as out_f:
    for path in files_to_process:
        if not os.path.isfile(path):
            print(f"‚ö† Dosya bulunamadƒ±: {path}")
            continue
        print(f"‚Üí ƒ∞≈üleniyor: {os.path.basename(path)}")
        with open(path, 'r', encoding='utf-8', errors='ignore') as in_f:
            for line in in_f:
                line = line.rstrip('\n')
                if not line:
                    continue
                out_f.write(json.dumps({"code": line}, ensure_ascii=False) + '\n')

print(f"\n‚úÖ Tamamlandƒ±! JSONL dosyan var:\n   {output_path}")

import os

file_path = '/content/drive/MyDrive/filtered_selected_embedded_code.jsonl'

# 1) Var mƒ±, Dosya mƒ±?
if os.path.exists(file_path) and os.path.isfile(file_path):
    size_bytes = os.path.getsize(file_path)
    print(f"‚úÖ Dosya bulundu: {file_path}")
    print(f"   Boyut: {size_bytes / (1024**2):.2f} MB")

    # 2) Satƒ±r sayƒ±sƒ±nƒ± da √∂ƒürenmek istersen:
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        line_count = sum(1 for _ in f)
    print(f"   Toplam satƒ±r: {line_count}")
else:
    print(f"‚ö† Dosya bulunamadƒ±: {file_path}")

!cp /content/drive/MyDrive/filtered_selected_embedded_code.jsonl /content/code_raw.jsonl

import re

INPUT_FILE = "/content/code_raw.jsonl"
OUTPUT_FILE = "/content/code_cleaned.jsonl"

LICENSE_START_PATTERNS = [
    r"\*{5,}", r"@file", r"@author", r"Copyright",
    r"licensed under", r"This software is licensed",
    r"AS-IS", r"All rights reserved"
]
license_start_regex = re.compile("|".join(LICENSE_START_PATTERNS), re.IGNORECASE)

def remove_license_at_end(code: str) -> str:
    lines = code.split("\n")
    cleaned_lines = []
    found_license = False
    for i in range(len(lines)-1, -1, -1):
        line = lines[i].strip()
        if not found_license and license_start_regex.search(line):
            found_license = True
            continue
        if found_license:
            continue
        cleaned_lines.insert(0, lines[i])
    return "\n".join(cleaned_lines).strip()

with open(INPUT_FILE, "r", encoding="utf-8") as infile, \
     open(OUTPUT_FILE, "w", encoding="utf-8") as outfile:

    kept, cleaned = 0, 0
    for idx, line in enumerate(infile):
        try:
            content = eval(line.strip()) if line.strip().startswith('"') else line.strip()
            cleaned_code = remove_license_at_end(content)
            if cleaned_code:
                outfile.write(cleaned_code + "\n")
                kept += 1
            else:
                cleaned += 1
        except Exception:
            cleaned += 1
        if idx % 200000 == 0:
            print(f"‚ñ∂Ô∏è {idx} satƒ±r i≈ülendi... (Tutulan: {kept}, Atƒ±lan: {cleaned})")

print(f"‚úÖ Temizleme tamamlandƒ±. Korunan: {kept}, Atƒ±lan: {cleaned}")

!cp /content/code_cleaned.jsonl /content/drive/MyDrive/cleaned_code_final.jsonl

import os

path = "/content/drive/MyDrive/cleaned_code_final.jsonl"

if os.path.exists(path):
    size_mb = os.path.getsize(path) / (1024*1024)
    with open(path, 'r', encoding='utf-8') as f:
        lines = sum(1 for _ in f)
    print(f"‚úÖ Dosya bulundu: {path}")
    print(f"üìè Boyut: {size_mb:.2f} MB")
    print(f"üìÑ Satƒ±r sayƒ±sƒ±: {lines}")
else:
    print("‚ùå Dosya bulunamadƒ±!")

!pip install transformers datasets --quiet

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoderbase-1b", use_fast=True)
tokenizer.pad_token = tokenizer.eos_token  # Bu √∂nemli!

!cp /content/drive/MyDrive/cleaned_code_final.jsonl /content/cleaned_code_final.jsonl

from datasets import Dataset

lines = []
with open("/content/cleaned_code_final.jsonl", "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        lines.append({"text": line.strip()})
        if i >= 500000:  # Sadece ilk 500k satƒ±rƒ± alƒ±yoruz, hƒ±zlƒ± test i√ßin
            break

dataset = Dataset.from_list(lines)
print(f"‚úÖ Dataset hazƒ±r: {len(dataset)} √∂rnek i√ßeriyor")

with open("/content/drive/MyDrive/cleaned_code_final.jsonl", "r", encoding="utf-8") as f:
    for i in range(500):
        line = f.readline()
        print(f"üîπ Satƒ±r {i+1}:\n{line}\n{'-'*60}")

import glob, os

# 1) VM‚Äôdeki par√ßa dosyalarƒ± doƒüru desenle yakala
parts = sorted(glob.glob('/content/filtered_embedded_code_*.txt'))
print("Found parts:", parts[:5], "... total:", len(parts))

# 2) Birle≈ütirilmi≈ü dosyanƒ±n Drive‚Äôdaki hedefi
merged_path = '/content/drive/MyDrive/filtered_embedded_code_combined.txt'

# 3) Merge i≈ülemi
with open(merged_path, 'w', encoding='utf-8') as out:
    for p in parts:
        with open(p, 'r', encoding='utf-8', errors='ignore') as inp:
            for line in inp:
                out.write(line)

# 4) Son durumu kontrol et
size = os.path.getsize(merged_path)
print(f"‚úÖ Merged to {merged_path} ‚Äî size: {size / (1024**2):.1f} MB")

import os

p = '/content/drive/MyDrive/filtered_embedded_code_combined.txt'
print("Exists:", os.path.exists(p))
if os.path.exists(p):
    size_mb = os.path.getsize(p) / 1024**2
    print(f"Size: {size_mb:.1f} MB")

!ls -lh

import os
from google.colab import files

# Upload your kaggle.json file
print("Please upload your kaggle.json file.")
uploaded = files.upload()

for fn in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes'.format(
        name=fn, length=len(uploaded[fn])))

# Create the .kaggle directory if it doesn't exist
!mkdir -p /root/.config/kaggle

# Move the uploaded kaggle.json file to the correct location
# Assuming the uploaded file is named 'kaggle.json'
if 'kaggle.json' in uploaded:
    !mv kaggle.json /root/.config/kaggle/kaggle.json
    print("kaggle.json moved to ~/.kaggle/")
else:
    print("kaggle.json not found in uploaded files.")

# Set permissions for the kaggle.json file
!chmod 600 /root/.config/kaggle/kaggle.json
print("Permissions set for kaggle.json")

# Now you can proceed with your Kaggle API operations
# from kaggle.api.kaggle_api_extended import KaggleApi
# api = KaggleApi()
# api.authenticate()
# ... your download code ...

# 1) Drive‚Äôƒ± mount et
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# 2) Gerekli k√ºt√ºphaneler
from datasets import load_from_disk, Dataset
import re, time, os

# 3) INPUT ve OUTPUT path‚Äôleri
INPUT_PATH  = "/content/drive/MyDrive/tokenized_combined_dataset"
OUTPUT_PATH = "/content/drive/MyDrive/filtered_tokenized_dataset"

# 4) Filtremizdeki ayarlar
LICENSE_INDICATORS = [
    'copyright','mit license','gnu general public license',
    'stmicroelectronics','evaluation board','eval board'
]
EMBEDDED_KEYWORDS = [
  'HAL_','GPIO_','UART_','I2C_','SPI_','TIM_','ADC_','DMA_',
  'STM32','GPIOA','GPIOB','GPIOC','SystemClock_Config',
  'digitalWrite','analogRead','pinMode','Serial.begin',
  'xTaskCreate','vTaskDelay','xQueue','FreeRTOS.h',
  'PORTA','DDRA','ISR(','avr/io.h','__delay_ms',
  'P1OUT','WDTCTL','volatile','uint8_t'
]
DESKTOP_EXCLUDES = [
    'printf(', 'scanf(', 'malloc(', 'free(',
    'main(int argc', 'windows.h', 'linux/', 'sys/',
    'iostream', 'std::cout'
]
MIN_LINES, MAX_LINES = 15, 400
MAX_HEADER_LINES = 30

# 5) Header temizleme ve embedded kontrol fonksiyonlarƒ±
def clean_header(code: str) -> str:
    lines = code.splitlines()
    cleaned, started = [], False
    for i, ln in enumerate(lines):
        s = ln.strip()
        if not started and i < MAX_HEADER_LINES:
            low = s.lower()
            if s.startswith(("//","/*","*","*/")): continue
            if any(ind in low for ind in LICENSE_INDICATORS): continue
            if s.startswith("#include") or re.match(r'^(?:void|int|static|uint\d+_t)\s+\w+', s):
                started = True
                cleaned.append(ln)
            else:
                continue
        else:
            cleaned.append(ln)
    return "\n".join(cleaned)

def is_embedded(code: str) -> bool:
    low = code.lower()
    if sum(p in low for p in DESKTOP_EXCLUDES) >= 2:
        return False
    return sum(kw.lower() in low for kw in EMBEDDED_KEYWORDS) >= 2

# 6) Disk‚Äôten y√ºkle
print("üîç Loading dataset from", INPUT_PATH)
ds = load_from_disk(INPUT_PATH)   # expects dataset_info.json + .arrow shards
print("‚úÖ Loaded. √ñrnek sayƒ±sƒ±:", len(ds))

# 7) Filtre fonksiyonu
def keep_example(example):
    code = example.get("text", "")    # eƒüer farklƒ± bir field adƒ±ysa ona g√∂re deƒüi≈ütirin
    cleaned = clean_header(code)
    nlines = cleaned.count("\n") + 1
    if not (MIN_LINES <= nlines <= MAX_LINES):
        return False
    if not is_embedded(cleaned):
        return False
    example["text"] = cleaned
    return True

# 8) Filtreyi uygula
print("üöß Applying filter‚Ä¶")
ds_filt = ds.filter(keep_example, batched=False)
print(f"‚úÖ Filtered. Yeni √∂rnek sayƒ±sƒ±: {len(ds_filt)}")

# 9) Sonu√ßlarƒ± kaydet
print("üíæ Saving to", OUTPUT_PATH)
ds_filt.save_to_disk(OUTPUT_PATH)
print("üéâ Done!")

from google.colab import drive
import os

# 1) Drive'ƒ± baƒüla (eƒüer zaten baƒülƒ±ysa force_remount=False yapabilirsiniz)
drive.mount('/content/drive', force_remount=True)

# 2) Dosya yolunu ayarla
dataset_path = "/content/drive/MyDrive/embedded_enhanced_dataset.jsonl"

# 3) Toplam √∂rnek sayƒ±sƒ±nƒ± ve √∂nizlemeyi al
with open(dataset_path, "r", encoding="utf-8") as f:
    lines = f.readlines()

print(f"üìä Toplam √∂rnek sayƒ±sƒ±: {len(lines)}\n")
print("üîç ƒ∞lk 5 √∂rnek (ilk 200 karakter):\n")
for i, line in enumerate(lines[:5], 1):
    print(f"{i}. {line[:200]}{'...' if len(line) > 200 else ''}")

from google.colab import files
files.upload()  # √áalƒ±≈ütƒ±rƒ±nca dosya se√ßme ekranƒ± a√ßƒ±lacak, oradan `kaggle.json` dosyanƒ± se√ß

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!pip install kagglehub

!unzip embedded-cc-raw-code.zip

import kagglehub

# Download and get path
path = kagglehub.dataset_download("muneebullah123/embedded-cc-raw-code")

print("Path to dataset files:", path)

import os

path = "/root/.cache/kagglehub/datasets/muneebullah123/embedded-cc-raw-code/versions/1"

for fname in os.listdir(path):
    print(fname)

import os

path = "/root/.cache/kagglehub/datasets/muneebullah123/embedded-cc-raw-code/versions/1"

for root, dirs, files in os.walk(path):
    for fname in files:
        print(os.path.join(root, fname))

import os
import re
import json
import hashlib
from collections import defaultdict, Counter
import time
from google.colab import drive
import gc  # Garbage collection

class ImprovedEmbeddedFilter:
    def __init__(self, dataset_path):
        self.dataset_path = dataset_path
        self.total_files = 0
        self.processed_files = 0
        self.accepted_count = 0
        self.duplicate_count = 0
        self.skipped_files = 0
        self.error_files = 0

        # Geli≈ütirilmi≈ü embedded pattern'ler - daha spesifik
        self.embedded_patterns = [
            # Hardware register patterns
            r'\b(PORT[A-L]|DDR[A-L]|PIN[A-L]|TRIS[A-E]|LAT[A-E])\s*[=&|]',
            r'\b(ADMUX|ADCSRA|TIMSK\d*|TCCR\d+[AB]?|TCNT\d+|OCR\d+[AB]?)\b',
            r'\b(MCUCR|GICR|GIFR|SREG|SPL|SPH)\b',

            # Microcontroller specific
            r'_delay_ms\s*\(\s*\d+\s*\)|_delay_us\s*\(\s*\d+\s*\)',
            r'__delay_ms\s*\(\s*\d+\s*\)|__delay_us\s*\(\s*\d+\s*\)',

            # Arduino/embedded functions
            r'\b(digitalWrite|digitalRead|pinMode|analogRead|analogWrite)\s*\(',
            r'\b(attachInterrupt|detachInterrupt|noInterrupts|interrupts)\s*\(',

            # Serial/UART communication
            r'Serial\.(print|println|begin|available|read)\s*\(',
            r'\b(UART|USART)\d*[_.]',

            # Interrupt handling
            r'ISR\s*\(\s*\w+_vect\s*\)',
            r'interrupt\s+\w+\s*\(',
            r'SIGNAL\s*\(\s*\w+\s*\)',

            # Memory and registers
            r'volatile\s+(uint8_t|uint16_t|uint32_t|char|int)\s+\w+',
            r'\bcli\s*\(\s*\)|\bsei\s*\(\s*\)',

            # ARM/STM32/ESP specific
            r'\b(GPIO[A-K]|HAL_\w+\(|esp_\w+)',
            r'\b(RCC|GPIOA|GPIOB|GPIOC|GPIOD|USART\d+|TIM\d+)\b',

            # AVR/embedded headers
            r'#include\s*<avr/\w+\.h>',
            r'#include\s*<util/delay\.h>',
            r'#include\s*<Arduino\.h>',

            # Typical embedded patterns
            r'while\s*\(\s*1\s*\)\s*\{|for\s*\(\s*;\s*;\s*\)\s*\{',
            r'\b(EEPROM|PROGMEM|FLASHEND)\b',

            # Bit manipulation (common in embedded)
            r'\w+\s*[|&^]\s*=\s*\(\s*1\s*<<\s*\d+\s*\)',
            r'\w+\s*[|&^]\s*=\s*_BV\s*\(',
        ]

        # Desktop/non-embedded eleme pattern'leri
        self.desktop_patterns = [
            r'#include\s*<iostream>\s*.*std::(cout|cin)',
            r'int\s+main\s*\(\s*int\s+argc\s*,\s*char\s*\*\*\s*argv\s*\)',
            r'malloc\s*\(.*free\s*\(',
            r'System\.out\.|Console\.Write',
            r'class\s+\w+.*:\s*public.*\{',
            r'std::(vector|map|list|set|queue|stack)',
            r'using\s+namespace\s+std',
            r'printf\s*\(\s*".*%[sd].*"',  # Desktop-style printf
            r'scanf\s*\(\s*".*%[sd].*"',   # Desktop-style scanf
        ]

    def mount_drive(self):
        """Google Drive mount"""
        print("üîÑ Google Drive mount ediliyor...")
        try:
            drive.mount('/content/drive')
            print("‚úÖ Drive mount edildi!")
            return True
        except Exception as e:
            print(f"‚ùå Drive hatasƒ±: {e}")
            return False

    def explore_directory_structure(self, path, max_depth=3, current_depth=0):
        """Directory yapƒ±sƒ±nƒ± ke≈üfet"""
        structure = {}
        if current_depth >= max_depth:
            return structure

        try:
            for item in os.listdir(path):
                item_path = os.path.join(path, item)
                if os.path.isdir(item_path):
                    structure[item] = {
                        'type': 'directory',
                        'path': item_path,
                        'contents': self.explore_directory_structure(item_path, max_depth, current_depth + 1)
                    }
                else:
                    structure[item] = {
                        'type': 'file',
                        'path': item_path,
                        'size': os.path.getsize(item_path) if os.path.exists(item_path) else 0
                    }
        except PermissionError:
            pass
        except Exception as e:
            print(f"‚ö†Ô∏è Directory okuma hatasƒ± {path}: {e}")

        return structure

    def find_all_code_files(self, base_path):
        """T√ºm kod dosyalarƒ±nƒ± bul - derin arama"""
        print("üîç T√ºm kod dosyalarƒ± aranƒ±yor...")
        code_files = []
        extensions = ['.c', '.cpp', '.h', '.hpp', '.ino', '.cc', '.cxx']
        dir_count = 0
        file_count = 0

        print(f"üìÇ Tarama ba≈ülƒ±yor: {base_path}")

        for root, dirs, files in os.walk(base_path):
            dir_count += 1
            if dir_count <= 10:  # ƒ∞lk 10 klas√∂r√º g√∂ster
                print(f"   üìÅ Taranan klas√∂r: {root}")
                print(f"      üìÑ ƒ∞√ßindeki dosyalar: {len(files)}")

                # ƒ∞lk birka√ß dosyayƒ± g√∂ster
                for i, file in enumerate(files[:5]):
                    print(f"         - {file}")
                if len(files) > 5:
                    print(f"         ... ve {len(files)-5} dosya daha")

            for file in files:
                file_count += 1
                if any(file.lower().endswith(ext) for ext in extensions):
                    file_path = os.path.join(root, file)
                    try:
                        size = os.path.getsize(file_path)
                        if len(code_files) < 5:  # ƒ∞lk 5 kod dosyasƒ±nƒ± detaylandƒ±r
                            print(f"   ‚úÖ Kod dosyasƒ± bulundu: {file} ({size} bytes)")
                        if 100 < size < 1000000:  # 100 byte - 1MB arasƒ±
                            code_files.append(file_path)
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è Dosya okuma hatasƒ± {file}: {e}")
                        continue

        print(f"üìä Toplam {dir_count} klas√∂r tarandƒ±")
        print(f"üìä Toplam {file_count} dosya bulundu")
        print(f"üìÅ {len(code_files)} kod dosyasƒ± tespit edildi")
        return code_files

    def is_embedded_code(self, content, filename=""):
        """Geli≈ümi≈ü embedded kod tespiti"""
        if len(content.strip()) < 50:
            return False, "too_short"

        # √áok uzun dosyalarƒ± atla (muhtemelen generated kod)
        if len(content) > 100000:
            return False, "too_long"

        # Embedded pattern skorlarƒ±
        embedded_score = 0
        matched_patterns = []

        for i, pattern in enumerate(self.embedded_patterns):
            matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
            if matches:
                embedded_score += len(matches[:3])  # Maximum 3 puan per pattern
                matched_patterns.append(f"pattern_{i}")

        # Desktop pattern skorlarƒ±
        desktop_score = 0
        for pattern in self.desktop_patterns:
            if re.search(pattern, content, re.IGNORECASE | re.MULTILINE):
                desktop_score += 2
                if desktop_score >= 4:  # Erken √ßƒ±kƒ±≈ü
                    return False, "desktop_code"

        # Dosya adƒ±ndan ipu√ßlarƒ±
        filename_bonus = 0
        fname = filename.lower()
        embedded_keywords = ['arduino', 'avr', 'stm32', 'esp', 'embedded', 'firmware',
                           'micro', 'sensor', 'led', 'motor', 'pwm', 'adc', 'timer']

        for keyword in embedded_keywords:
            if keyword in fname:
                filename_bonus += 1

        # Karar verme mantƒ±ƒüƒ±
        total_embedded_score = embedded_score + filename_bonus

        if total_embedded_score >= 5 and desktop_score <= 2:
            return True, f"embedded_score_{total_embedded_score}_patterns_{len(matched_patterns)}"
        elif total_embedded_score >= 3 and desktop_score == 0:
            return True, f"medium_embedded_score_{total_embedded_score}"
        elif filename_bonus >= 2 and total_embedded_score >= 2:
            return True, f"filename_bonus_{filename_bonus}_score_{total_embedded_score}"

        return False, f"low_score_emb_{total_embedded_score}_desk_{desktop_score}"

    def content_hash(self, content):
        """ƒ∞√ßerik hash'i - duplicate tespiti i√ßin"""
        # ƒ∞lk 500 karakter + son 200 karakter
        sample = content[:500] + content[-200:] if len(content) > 700 else content
        sample = re.sub(r'\s+', ' ', sample.strip())
        sample = re.sub(r'//.*?$', '', sample, flags=re.MULTILINE)  # Comment'larƒ± kaldƒ±r
        return hashlib.md5(sample.encode()).hexdigest()[:16]

    def clean_code(self, content):
        """Kod temizleme"""
        lines = content.split('\n')
        cleaned_lines = []
        empty_count = 0

        for line in lines:
            stripped = line.rstrip()

            # Bo≈ü satƒ±rlarƒ± kontrol et
            if not stripped:
                if empty_count == 0:
                    cleaned_lines.append('')
                    empty_count = 1
                continue

            # Tek satƒ±r comment'larƒ± atla
            if stripped.startswith('//') or stripped.startswith('*') or stripped.startswith('/*'):
                continue

            cleaned_lines.append(stripped)
            empty_count = 0

        # Son temizlik
        result = '\n'.join(cleaned_lines).strip()
        result = re.sub(r'\n{3,}', '\n\n', result)  # √áoklu bo≈ü satƒ±rlarƒ± d√ºzelt

        return result

    def process_files_batch(self, files_batch, seen_hashes, output_file):
        """Dosya batch'ini i≈üle"""
        batch_accepted = 0

        for file_path in files_batch:
            try:
                # Dosya boyutu kontrol√º
                file_size = os.path.getsize(file_path)
                if file_size > 500000:  # 500KB'dan b√ºy√ºk
                    self.skipped_files += 1
                    continue

                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                # DEBUG: ƒ∞lk birka√ß dosyanƒ±n i√ßeriƒüini g√∂ster
                if self.processed_files < 3:
                    print(f"\nüîç DEBUG - Dosya analizi: {os.path.basename(file_path)}")
                    print(f"   üìè Boyut: {len(content)} karakter")
                    print(f"   üìÑ ƒ∞lk 200 karakter:")
                    print(f"   {repr(content[:200])}")
                    print(f"   üìù ƒ∞√ßerik √∂nizleme:")
                    preview_lines = content.split('\n')[:10]
                    for i, line in enumerate(preview_lines):
                        print(f"   {i+1:2d}: {line[:60]}...")

                # Embedded kontrol√º
                is_embedded, reason = self.is_embedded_code(content, os.path.basename(file_path))

                if self.processed_files < 3:
                    print(f"   üéØ Embedded test sonucu: {is_embedded} - {reason}")

                if is_embedded:
                    # Hash kontrol√º
                    content_hash = self.content_hash(content)
                    if content_hash not in seen_hashes:
                        seen_hashes.add(content_hash)

                        # Kod temizleme
                        cleaned = self.clean_code(content)

                        if len(cleaned) >= 100:  # Minimum uzunluk kontrol√º
                            # JSON entry olu≈ütur
                            entry = {
                                "content": cleaned,
                                "file_name": os.path.basename(file_path),
                                "file_path": file_path,
                                "length": len(cleaned),
                                "reason": reason,
                                "original_size": file_size
                            }

                            json.dump(entry, output_file, ensure_ascii=False)
                            output_file.write('\n')
                            output_file.flush()

                            self.accepted_count += 1
                            batch_accepted += 1

                            print(f"‚úÖ {self.accepted_count}: {os.path.basename(file_path)} ({len(cleaned)} chars) - {reason}")

                    else:
                        self.duplicate_count += 1

                self.processed_files += 1

            except Exception as e:
                self.error_files += 1
                if self.error_files <= 5:  # ƒ∞lk 5 hatayƒ± g√∂ster
                    print(f"‚ö†Ô∏è Hata: {os.path.basename(file_path)} - {str(e)[:50]}")

        return batch_accepted

    def process_all_files(self, batch_size=30):
        """Ana i≈üleme fonksiyonu"""
        print("üöÄ Geli≈ümi≈ü embedded filtreleme ba≈ülƒ±yor...")
        start_time = time.time()

        # T√ºm kod dosyalarƒ±nƒ± bul
        all_files = self.find_all_code_files(self.dataset_path)
        self.total_files = len(all_files)

        if self.total_files == 0:
            print("‚ùå Hi√ß kod dosyasƒ± bulunamadƒ±!")
            return None

        print(f"üìä Toplam {self.total_files:,} kod dosyasƒ± bulundu")

        # Output dosyasƒ±
        output_path = "embedded_codes_filtered.jsonl"
        seen_hashes = set()

        print(f"üíæ √áƒ±ktƒ± dosyasƒ±: {output_path}")
        print("-" * 50)

        with open(output_path, 'w', encoding='utf-8') as output_file:
            # Batch'ler halinde i≈üle
            for i in range(0, len(all_files), batch_size):
                batch = all_files[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(all_files) + batch_size - 1) // batch_size

                print(f"\nüîÑ Batch {batch_num}/{total_batches} i≈üleniyor... ({len(batch)} dosya)")

                batch_accepted = self.process_files_batch(batch, seen_hashes, output_file)

                # ƒ∞lerleme raporu
                if self.processed_files > 0:
                    elapsed = time.time() - start_time
                    progress = (i + len(batch)) / len(all_files) * 100
                    rate = self.processed_files / elapsed if elapsed > 0 else 0
                    success_rate = (self.accepted_count / self.processed_files) * 100
                    eta = (len(all_files) - self.processed_files) / rate / 60 if rate > 0 else 0

                    print(f"üìà ƒ∞lerleme: {progress:.1f}% | Kabul: {self.accepted_count:,} | "
                          f"Ba≈üarƒ±: %{success_rate:.1f} | Hƒ±z: {rate:.1f}/sn | Kalan: {eta:.1f}dk")

                # Memory cleanup
                if batch_num % 5 == 0:
                    gc.collect()

        # Final rapor
        total_time = time.time() - start_time
        self.print_final_report(total_time, output_path)

        return output_path

    def print_final_report(self, total_time, output_path):
        """Final raporu yazdƒ±r"""
        print(f"\n{'='*60}")
        print(f"üéâ EMBEDDED C DATASET Fƒ∞LTRELEME TAMAMLANDI!")
        print(f"{'='*60}")
        print(f"‚è±Ô∏è  Toplam s√ºre: {total_time/60:.1f} dakika")
        print(f"üìÅ Bulunan dosya: {self.total_files:,}")
        print(f"‚öôÔ∏è  ƒ∞≈ülenen dosya: {self.processed_files:,}")
        print(f"‚úÖ Kabul edilen: {self.accepted_count:,}")
        print(f"üîÑ Duplicate: {self.duplicate_count:,}")
        print(f"‚è≠Ô∏è  Atlanan (b√ºy√ºk): {self.skipped_files:,}")
        print(f"‚ùå Hatalƒ±: {self.error_files:,}")

        if self.processed_files > 0:
            success_rate = (self.accepted_count / self.processed_files) * 100
            print(f"üìä Ba≈üarƒ± oranƒ±: %{success_rate:.1f}")

        print(f"üíæ √áƒ±ktƒ± dosyasƒ±: {output_path}")

        # Dosya boyutu
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path) / (1024*1024)
            print(f"üì¶ Dosya boyutu: {file_size:.1f} MB")

    def save_to_drive(self, local_file, drive_folder="/content/drive/MyDrive/embedded_dataset/"):
        """Drive'a kaydet"""
        if not os.path.exists(local_file):
            print("‚ùå Kaydedilecek dosya bulunamadƒ±!")
            return False

        try:
            os.makedirs(drive_folder, exist_ok=True)

            import shutil
            drive_path = os.path.join(drive_folder, "embedded_codes_final.jsonl")
            shutil.copy2(local_file, drive_path)

            # Stats dosyasƒ±
            stats_path = os.path.join(drive_folder, "dataset_summary.txt")
            with open(stats_path, 'w', encoding='utf-8') as f:
                f.write("EMBEDDED C DATASET SUMMARY\n")
                f.write("="*40 + "\n\n")
                f.write(f"Total files found: {self.total_files:,}\n")
                f.write(f"Files processed: {self.processed_files:,}\n")
                f.write(f"Accepted files: {self.accepted_count:,}\n")
                f.write(f"Duplicate files: {self.duplicate_count:,}\n")
                f.write(f"Skipped (large): {self.skipped_files:,}\n")
                f.write(f"Error files: {self.error_files:,}\n")

                if self.processed_files > 0:
                    success_rate = (self.accepted_count / self.processed_files) * 100
                    f.write(f"Success rate: {success_rate:.1f}%\n")

                f.write(f"\nOutput file: embedded_codes_final.jsonl\n")

            print(f"üíæ Drive'a kaydedildi:")
            print(f"   üìÑ Data: {drive_path}")
            print(f"   üìä Stats: {stats_path}")
            return True

        except Exception as e:
            print(f"‚ùå Drive kaydetme hatasƒ±: {e}")
            return False

    def show_samples(self, jsonl_file, count=3):
        """Dataset √∂rneklerini g√∂ster"""
        if not os.path.exists(jsonl_file):
            print("‚ùå √ñrnek dosya bulunamadƒ±!")
            return

        print(f"\nüìã ƒ∞LK {count} EMBEDDED C √ñRNEƒûƒ∞:")
        print("="*70)

        try:
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= count:
                        break

                    entry = json.loads(line)
                    print(f"\nüîπ √ñrnek {i+1}: {entry['file_name']}")
                    print(f"   üìè Boyut: {entry['length']} karakter")
                    print(f"   üéØ Tespit nedeni: {entry['reason']}")
                    print(f"   üìÅ Yol: ...{entry['file_path'][-50:]}")
                    print("-" * 50)

                    # Code preview
                    content = entry['content']
                    preview = content[:300] + "\n..." if len(content) > 300 else content
                    print(preview)
                    print("-" * 50)

        except Exception as e:
            print(f"‚ùå √ñrnek g√∂sterme hatasƒ±: {e}")

def main():
    # Dataset ana yolu
    base_path = "/root/.cache/kagglehub/datasets/muneebullah123/embedded-cc-raw-code/versions/1"

    print("üöÄ EMBEDDED C DATASET Fƒ∞LTRELEME ARACI")
    print("="*50)

    # Filter objesi olu≈ütur
    filter_obj = ImprovedEmbeddedFilter(base_path)

    # Drive mount
    drive_mounted = filter_obj.mount_drive()

    print(f"\nüìÅ Dataset yolu: {base_path}")

    # Directory yapƒ±sƒ±nƒ± ke≈üfet
    print("\nüîç Dataset yapƒ±sƒ± ke≈üfediliyor...")
    structure = filter_obj.explore_directory_structure(base_path, max_depth=2)

    print(f"üìÇ Ana klas√∂rde {len(structure)} √∂ƒüe bulundu:")
    for name, info in list(structure.items())[:10]:  # ƒ∞lk 10'u g√∂ster
        if info['type'] == 'directory':
            print(f"   üìÅ {name}/")
        else:
            size_mb = info['size'] / (1024*1024) if info['size'] > 0 else 0
            print(f"   üìÑ {name} ({size_mb:.1f} MB)")

    # Ana filtreleme i≈ülemi
    print(f"\nüéØ Ana filtreleme ba≈ülƒ±yor...")
    output_file = filter_obj.process_all_files(batch_size=25)

    if output_file and filter_obj.accepted_count > 0:
        # √ñrnekleri g√∂ster
        filter_obj.show_samples(output_file, count=3)

        # Drive'a kaydet
        if drive_mounted:
            filter_obj.save_to_drive(output_file)

        print(f"\nüéâ BA≈ûARILI! {filter_obj.accepted_count:,} embedded C kodu filtrelendi!")

    else:
        print(f"\n‚ùå Hi√ß embedded kod tespit edilemedi!")
        print("üîç Muhtemel nedenler:")
        print("   - Pattern'ler √ßok kƒ±sƒ±tlayƒ±cƒ± olabilir")
        print("   - Dosya yollarƒ± yanlƒ±≈ü olabilir")
        print("   - Dosya formatlarƒ± beklenenden farklƒ± olabilir")

if __name__ == "__main__":
    main()

!find /root/.cache/kagglehub/datasets/muneebullah123/embedded-cc-raw-code/versions/1 -type f | wc -l

!pip install kagglehub --upgrade
import kagglehub

# Dataset'i indir
path = kagglehub.dataset_download("muneebullah123/embedded-cc-raw-code")
print("‚úÖ Dataset dizini:", path)

!find /root/.cache/kagglehub/datasets/muneebullah123/embedded-cc-raw-code/versions/1 -type f | head -20

!head -n 50 /root/.cache/kagglehub/datasets/muneebullah123/embedded-cc-raw-code/versions/1/embedded-c-p1/embedded-c-p1/all.c

import os

root_path = "/root/.cache/kagglehub/datasets/muneebullah123/embedded-cc-raw-code/versions/1"
counter = 0

for dirpath, _, filenames in os.walk(root_path):
    for fname in filenames:
        if fname.endswith(('.c', '.cpp', '.h', '.hpp', '.cxx', '.hxx')):
            fpath = os.path.join(dirpath, fname)
            print(f"üìÇ Dosya: {fpath}")
            with open(fpath, 'r', encoding='utf-8', errors='ignore') as f:
                print(f.read(400))  # ƒ∞lk 400 karakteri g√∂ster
            print("\n" + "-"*80 + "\n")
            counter += 1
            if counter >= 3:
                break
    if counter >= 3:
        break

