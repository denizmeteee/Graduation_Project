# -*- coding: utf-8 -*-
"""Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1csT1bMNJ2kyZ168risCQGygSgZhHKRo_
"""

# Gerekli kütüphaneler
!pip install -q transformers datasets peft accelerate bitsandbytes

# Google Drive'ı mount et
from google.colab import drive
drive.mount('/content/drive')

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from datasets import load_from_disk
from peft import prepare_model_for_kbit_training

# 📁 Yol bilgileri
model_path = "/content/drive/MyDrive/huggingface_cache/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11"
dataset_path = "/content/drive/MyDrive/tokenized_combined_dataset"

# ✅ Tokenizer yükle
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

# ✅ 4-bit quantization ayarları
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16"
)

# ✅ Model yükle (4-bit + QLoRA için hazırla)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    quantization_config=bnb_config,
    trust_remote_code=True
)
model = prepare_model_for_kbit_training(model)

# ✅ Dataset yükle (tek split dataset)
dataset = load_from_disk(dataset_path)

# ✅ Train/Validation split
split_dataset = dataset.train_test_split(test_size=0.05, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

print("✅ Tüm bileşenler başarıyla yüklendi.")

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_proj", "q_attn", "c_attn", "c_fc"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/final_finetuned_model",
    per_device_train_batch_size=6,
    gradient_accumulation_steps=2,
    num_train_epochs=2,
    learning_rate=1e-4,
    lr_scheduler_type="cosine",
    warmup_steps=100,
    fp16=True,

    eval_strategy="steps",          # ✅ Artık adım bazlı validation
    eval_steps=1000,                      # ✅ Her 1000 adımda bir validation yap
    save_strategy="steps",                # ✅ Her 1000 adımda checkpoint al
    save_steps=1000,                      # ✅ Checkpoint sıklığı
    save_total_limit=5,                   # ✅ Maksimum 5 checkpoint tut (disk tasarrufu)

    logging_steps=50,
    logging_dir="./logs",
    report_to="tensorboard",
    run_name="starcoder_finetune_v1",

    push_to_hub=False,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False}
)

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, TrainerCallback
import os
from datetime import datetime
import torch

# 🔧 Ortam ayarları
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["WANDB_DISABLED"] = "true"

# ✅ Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# ✅ Özel callback ile training loglarını net göster
class PrintLossCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        print(f"📊 Epoch {state.epoch:.2f} tamamlandı | "
              f"Train Loss: {state.log_history[-1].get('loss', 'N/A')} | "
              f"Eval Loss: {state.log_history[-1].get('eval_loss', 'N/A')}")

# ✅ Eğitim ayarları
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/final_finetuned_model",
    per_device_train_batch_size=6,                    # ⚡ GPU doluluğu optimize
    gradient_accumulation_steps=2,                    # ⬆️ Sanal batch = 12
    num_train_epochs=2,
    learning_rate=1e-4,
    lr_scheduler_type="cosine",
    warmup_steps=100,
    fp16=True,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    logging_steps=50,
    logging_dir="./logs",
    report_to="none",
    push_to_hub=False,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
    run_name="starcoder_finetune_v1",          # W&B, TensorBoard, log dosyası adı
    load_best_model_at_end=True,               # En iyi eval_loss'lu modeli yükle
    metric_for_best_model="eval_loss",         # Değerlendirme metrik olarak loss
    greater_is_better=False                    # Düşük loss daha iyidir
)

# ✅ Trainer nesnesi
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[PrintLossCallback()]
)

# ⏱️ Eğitim başlat
start = datetime.now()
trainer.train(resume_from_checkpoint="/content/drive/MyDrive/final_finetuned_model/checkpoint-5538")
print("✅ Eğitim tamamlandı! ⏱️ Geçen süre:", datetime.now() - start)

import torch
torch.cuda.empty_cache()

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig

# ✅ Tokenizer'ı base modelden al (çünkü adapter klasöründe yok)
base_model_path = "/content/drive/MyDrive/huggingface_cache/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11"
tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token  # ✅ Bunu buraya ekle!

# ✅ PEFT config dosyasını adapter klasöründen al
adapter_path = "/content/drive/MyDrive/final_finetuned_model/checkpoint-5538"
config = PeftConfig.from_pretrained(adapter_path)

# ✅ Base modeli tekrar yükle
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    device_map="auto",
    trust_remote_code=True
)

# ✅ Adapter'ı modelele entegre et
model = PeftModel.from_pretrained(model, adapter_path)

print("✅ Model başarıyla yüklendi ve adapter entegre edildi.")

model.save_pretrained("/content/drive/MyDrive/final_full_model")
tokenizer.save_pretrained("/content/drive/MyDrive/final_full_model")

model.save_pretrained("/content/drive/MyDrive/final_full_model")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Tokenizer ve modelin yolu
model_dir = "/content/drive/MyDrive/final_full_model"

# Tokenizer'ı yükle
tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM

# PEFT (LoRA) config'i oku
config = PeftConfig.from_pretrained(model_dir)

# Base modeli yükle
base_model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    trust_remote_code=True,
    device_map="auto"
)

# Adapter'ı entegre et
model = PeftModel.from_pretrained(base_model, model_dir)
model.eval()

# Örnek prompt
prompt = "/* Calculate factorial of a number using recursion */\n"

# Tokenize et
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Kod üret
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7
    )

# Çıktıyı göster
generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_code)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

metrics = trainer.evaluate()
print("✅ Evaluation tamamlandı. Eval loss:", metrics["eval_loss"])

import matplotlib.pyplot as plt
import json

# ✅ Trainer log dosyasının doğru yolu
log_path = "/content/drive/MyDrive/final_finetuned_model/checkpoint-5538/trainer_state.json"

with open(log_path, "r") as f:
    logs = json.load(f)

# ✅ Epoch başına loss'ları çıkar
train_losses = []
eval_losses = []
epochs_train = []
epochs_eval = []

for log in logs["log_history"]:
    if "loss" in log and "epoch" in log:
        train_losses.append(log["loss"])
        epochs_train.append(log["epoch"])
    if "eval_loss" in log and "epoch" in log:
        eval_losses.append(log["eval_loss"])
        epochs_eval.append(log["epoch"])

# ✅ Grafik çizimi
plt.figure(figsize=(10, 6))
plt.plot(epochs_train, train_losses, label="Training Loss", marker="o")
plt.plot(epochs_eval, eval_losses, label="Validation Loss", marker="s", color="orange")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training & Validation Loss per Epoch")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()