# -*- coding: utf-8 -*-
"""Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1csT1bMNJ2kyZ168risCQGygSgZhHKRo_
"""

# Gerekli kÃ¼tÃ¼phaneler
!pip install -q transformers datasets peft accelerate bitsandbytes

# Google Drive'Ä± mount et
from google.colab import drive
drive.mount('/content/drive')

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from datasets import load_from_disk
from peft import prepare_model_for_kbit_training

# ğŸ“ Yol bilgileri
model_path = "/content/drive/MyDrive/huggingface_cache/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11"
dataset_path = "/content/drive/MyDrive/tokenized_combined_dataset"

# âœ… Tokenizer yÃ¼kle
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

# âœ… 4-bit quantization ayarlarÄ±
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16"
)

# âœ… Model yÃ¼kle (4-bit + QLoRA iÃ§in hazÄ±rla)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    quantization_config=bnb_config,
    trust_remote_code=True
)
model = prepare_model_for_kbit_training(model)

# âœ… Dataset yÃ¼kle (tek split dataset)
dataset = load_from_disk(dataset_path)

# âœ… Train/Validation split
split_dataset = dataset.train_test_split(test_size=0.05, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

print("âœ… TÃ¼m bileÅŸenler baÅŸarÄ±yla yÃ¼klendi.")

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_proj", "q_attn", "c_attn", "c_fc"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/final_finetuned_model",
    per_device_train_batch_size=6,
    gradient_accumulation_steps=2,
    num_train_epochs=2,
    learning_rate=1e-4,
    lr_scheduler_type="cosine",
    warmup_steps=100,
    fp16=True,

    eval_strategy="steps",          # âœ… ArtÄ±k adÄ±m bazlÄ± validation
    eval_steps=1000,                      # âœ… Her 1000 adÄ±mda bir validation yap
    save_strategy="steps",                # âœ… Her 1000 adÄ±mda checkpoint al
    save_steps=1000,                      # âœ… Checkpoint sÄ±klÄ±ÄŸÄ±
    save_total_limit=5,                   # âœ… Maksimum 5 checkpoint tut (disk tasarrufu)

    logging_steps=50,
    logging_dir="./logs",
    report_to="tensorboard",
    run_name="starcoder_finetune_v1",

    push_to_hub=False,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False}
)

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, TrainerCallback
import os
from datetime import datetime
import torch

# ğŸ”§ Ortam ayarlarÄ±
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["WANDB_DISABLED"] = "true"

# âœ… Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# âœ… Ã–zel callback ile training loglarÄ±nÄ± net gÃ¶ster
class PrintLossCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        print(f"ğŸ“Š Epoch {state.epoch:.2f} tamamlandÄ± | "
              f"Train Loss: {state.log_history[-1].get('loss', 'N/A')} | "
              f"Eval Loss: {state.log_history[-1].get('eval_loss', 'N/A')}")

# âœ… EÄŸitim ayarlarÄ±
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/final_finetuned_model",
    per_device_train_batch_size=6,                    # âš¡ GPU doluluÄŸu optimize
    gradient_accumulation_steps=2,                    # â¬†ï¸ Sanal batch = 12
    num_train_epochs=2,
    learning_rate=1e-4,
    lr_scheduler_type="cosine",
    warmup_steps=100,
    fp16=True,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    logging_steps=50,
    logging_dir="./logs",
    report_to="none",
    push_to_hub=False,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
    run_name="starcoder_finetune_v1",          # W&B, TensorBoard, log dosyasÄ± adÄ±
    load_best_model_at_end=True,               # En iyi eval_loss'lu modeli yÃ¼kle
    metric_for_best_model="eval_loss",         # DeÄŸerlendirme metrik olarak loss
    greater_is_better=False                    # DÃ¼ÅŸÃ¼k loss daha iyidir
)

# âœ… Trainer nesnesi
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[PrintLossCallback()]
)

# â±ï¸ EÄŸitim baÅŸlat
start = datetime.now()
trainer.train(resume_from_checkpoint="/content/drive/MyDrive/final_finetuned_model/checkpoint-5538")
print("âœ… EÄŸitim tamamlandÄ±! â±ï¸ GeÃ§en sÃ¼re:", datetime.now() - start)

import torch
torch.cuda.empty_cache()

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig

# âœ… Tokenizer'Ä± base modelden al (Ã§Ã¼nkÃ¼ adapter klasÃ¶rÃ¼nde yok)
base_model_path = "/content/drive/MyDrive/huggingface_cache/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11"
tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token  # âœ… Bunu buraya ekle!

# âœ… PEFT config dosyasÄ±nÄ± adapter klasÃ¶rÃ¼nden al
adapter_path = "/content/drive/MyDrive/final_finetuned_model/checkpoint-5538"
config = PeftConfig.from_pretrained(adapter_path)

# âœ… Base modeli tekrar yÃ¼kle
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    device_map="auto",
    trust_remote_code=True
)

# âœ… Adapter'Ä± modelele entegre et
model = PeftModel.from_pretrained(model, adapter_path)

print("âœ… Model baÅŸarÄ±yla yÃ¼klendi ve adapter entegre edildi.")

model.save_pretrained("/content/drive/MyDrive/final_full_model")
tokenizer.save_pretrained("/content/drive/MyDrive/final_full_model")

model.save_pretrained("/content/drive/MyDrive/final_full_model")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Tokenizer ve modelin yolu
model_dir = "/content/drive/MyDrive/final_full_model"

# Tokenizer'Ä± yÃ¼kle
tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM

# PEFT (LoRA) config'i oku
config = PeftConfig.from_pretrained(model_dir)

# Base modeli yÃ¼kle
base_model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    trust_remote_code=True,
    device_map="auto"
)

# Adapter'Ä± entegre et
model = PeftModel.from_pretrained(base_model, model_dir)
model.eval()

# Ã–rnek prompt
prompt = "/* Calculate factorial of a number using recursion */\n"

# Tokenize et
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Kod Ã¼ret
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7
    )

# Ã‡Ä±ktÄ±yÄ± gÃ¶ster
generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_code)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

metrics = trainer.evaluate()
print("âœ… Evaluation tamamlandÄ±. Eval loss:", metrics["eval_loss"])

import matplotlib.pyplot as plt
import json

# âœ… Trainer log dosyasÄ±nÄ±n doÄŸru yolu
log_path = "/content/drive/MyDrive/final_finetuned_model/checkpoint-5538/trainer_state.json"

with open(log_path, "r") as f:
    logs = json.load(f)

# âœ… Epoch baÅŸÄ±na loss'larÄ± Ã§Ä±kar
train_losses = []
eval_losses = []
epochs_train = []
epochs_eval = []

for log in logs["log_history"]:
    if "loss" in log and "epoch" in log:
        train_losses.append(log["loss"])
        epochs_train.append(log["epoch"])
    if "eval_loss" in log and "epoch" in log:
        eval_losses.append(log["eval_loss"])
        epochs_eval.append(log["epoch"])

# âœ… Grafik Ã§izimi
plt.figure(figsize=(10, 6))
plt.plot(epochs_train, train_losses, label="Training Loss", marker="o")
plt.plot(epochs_eval, eval_losses, label="Validation Loss", marker="s", color="orange")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training & Validation Loss per Epoch")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()